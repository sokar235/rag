# ğŸ§  Retrieval-Augmented Generation (RAG) System with Local Vector Store

This project implements a complete Retrieval-Augmented Generation (RAG) pipeline using open-source tools and integrates with large language models (LLMs) from **NGU** and **GROQ** via custom API endpoints. The system loads local documents, processes them into chunks, embeds them using sentence-transformers, stores them using FAISS, and retrieves the most relevant content to augment language generation.

---

## ğŸ“¦ Features

- ğŸ” Loads and processes local `.pdf`, `.txt`, and `.docx` documents
- âœ‚ï¸ Chunks documents with overlap handling
- ğŸ§  Embeds text using HuggingFace transformers
- ğŸ’¾ Stores embeddings in FAISS for fast retrieval
- ğŸ¤– Integrates with GROQ or NGU LLMs via `.env` config
- ğŸ§µ Custom RAG pipeline using prompt injection into chat completion API

---

## ğŸ“ Folder Structure
rag_project/
â”œâ”€â”€ documents/ # Your source documents (PDF, TXT, DOCX)
â”œâ”€â”€ rag_code/
â”‚ â”œâ”€â”€ load_documents.py # Loads files from disk
â”‚ â”œâ”€â”€ split_documents.py # Chunks text with overlap
â”‚ â”œâ”€â”€ vector_store.py # Embeds and saves to FAISS
â”‚ â”œâ”€â”€ rag_pipeline.py # Custom retriever + LLM integration
â”œâ”€â”€ main.py # Entry point for running RAG query
â”œâ”€â”€ .env # API keys and model configs
â”œâ”€â”€ README.md # This file
â”œâ”€â”€ requirements.txt # All dependencies

yaml
Copy
Edit

---

## ğŸ§ª Setup Instructions

### 1. Clone & Create Virtual Environment
```bash
python -m venv venv
venv\Scripts\activate      # Windows
pip install -r requirements.txt


# NGU
NGU_API_KEY="#########"
NGU_BASE_URL="https://ngullama.femtoid.com/v1"
NGU_MODEL="qwen2.5-coder:7b"

# GROQ
GROQ_API_KEY="your-groq-api-key-here"
GROQ_BASE_URL="https://api.groq.com/openai/v1"
GROQ_MODEL="llama-3.3-70b-versatile"

# Select which provider to use
MODEL_SERVER="NGU"


 Run the System
bash
Copy
Edit
python main.py



"What is retrieval-augmented generation?"

